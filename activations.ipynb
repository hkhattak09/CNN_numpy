{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca9cd50",
   "metadata": {},
   "source": [
    "# Activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aec2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa0cf3",
   "metadata": {},
   "source": [
    "## Activations are represented as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "522f548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.out_vector = None\n",
    "    def forward(self,input_vector):\n",
    "        out_vec = np.where(input_vector>0,input_vector,0)\n",
    "        self.out_vector = out_vec\n",
    "        return out_vec\n",
    "    def backward(self):\n",
    "        gradient = np.where(self.out_vector>0,1,0)\n",
    "        return gradient\n",
    "\n",
    "\n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.out_vec = None\n",
    "    def forward(self,in_vec):\n",
    "        in_vec = np.clip(in_vec,-500,500)\n",
    "        out_vec = 1/(1+np.exp(-in_vec))\n",
    "        self.out_vec = out_vec\n",
    "        return out_vec\n",
    "    def backward(self):\n",
    "        gradient = self.out_vec*(1-self.out_vec)\n",
    "        return gradient\n",
    "\n",
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        self.out_vec = None\n",
    "    def forward(self,in_vec):\n",
    "        in_vec = np.clip(in_vec,-500,500)\n",
    "        out_vec = np.tanh(in_vec)\n",
    "        self.out_vec = out_vec\n",
    "        return out_vec\n",
    "    def backward(self):\n",
    "        gradient = 1-np.square(self.out_vec)\n",
    "        return gradient\n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.out_vec = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #softmax is rowwise\n",
    "        x = x-np.max(x,axis=1,keepdims=True)\n",
    "        expX = np.exp(x)\n",
    "        expSum = np.sum(expX,axis=1,keepdims=True)\n",
    "        self.out_vec = expX/expSum\n",
    "        return self.out_vec\n",
    "    \n",
    "    def backward(self):\n",
    "        #Jacobians = tensor of shape batch_size,num_classes,num_classes\n",
    "        Jacobians = np.zeros(shape=(self.out_vec.shape[0],self.out_vec.shape[1],self.out_vec.shape[1]))\n",
    "        # mask with ones everywhere except on the diagonal. its basicall a matrix of 1s - a diagonal matrix of 1s. eye is a numpy function that creates such a matrix.\n",
    "        mask = np.ones(shape=(self.out_vec.shape[1],self.out_vec.shape[1])) - np.eye(self.out_vec.shape[1],self.out_vec.shape[1])\n",
    "        for x in range(self.out_vec.shape[0]):\n",
    "            off_diagonal = np.outer(self.out_vec[x],-self.out_vec[x])\n",
    "            off_diagonal = off_diagonal*mask\n",
    "            diagonal = np.diag(self.out_vec[x]*(1-self.out_vec[x]))\n",
    "            Jacobian = off_diagonal + diagonal\n",
    "            Jacobians[x] = Jacobian\n",
    "        return Jacobians\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeaa004",
   "metadata": {},
   "source": [
    "## Testing activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe6e74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "#checking shapes of forward and backward pass\n",
    "ReLU_inst = ReLU()\n",
    "Tanh_inst = Tanh()\n",
    "Sigmoid_inst = Sigmoid()\n",
    "Softmax_inst = Softmax()\n",
    "actviations = [ReLU_inst,Tanh_inst,Sigmoid_inst,Softmax_inst]\n",
    "input_data = np.array([[1,2,3],[4,5,6]])\n",
    "forward_shape = (2,3)\n",
    "for idx,actviation in enumerate(actviations):\n",
    "    activation_shape = actviation.forward(input_data).shape\n",
    "    print(activation_shape)\n",
    "    if activation_shape != forward_shape:\n",
    "        print(f'{idx} has incorrect shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c814229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "for idx,actviation in enumerate(actviations):\n",
    "    activation_shape = actviation.backward().shape\n",
    "    print(activation_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f592c",
   "metadata": {},
   "source": [
    "### Expected shapes of forward and backward pass as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5242d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid\n",
      "Expected = [[0.73105858 0.88079708 0.95257413],[0.98201379 0.99330715 0.99752738]]\n",
      "Result = [[0.73105858 0.88079708 0.95257413]\n",
      " [0.98201379 0.99330715 0.99752738]]\n"
     ]
    }
   ],
   "source": [
    "#testing expected values\n",
    "print('Sigmoid')\n",
    "print('Expected = [[0.73105858 0.88079708 0.95257413],[0.98201379 0.99330715 0.99752738]]')\n",
    "print(f'Result = {Sigmoid_inst.forward(input_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d8f0c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanh\n",
      "Expected = [[0.76159416 0.96402758 0.99505475],[0.99932930 0.99990920 0.99998771]]\n",
      "Result = [[0.76159416 0.96402758 0.99505475]\n",
      " [0.9993293  0.9999092  0.99998771]]\n"
     ]
    }
   ],
   "source": [
    "print('Tanh')\n",
    "print('Expected = [[0.76159416 0.96402758 0.99505475],[0.99932930 0.99990920 0.99998771]]')\n",
    "print(f'Result = {Tanh_inst.forward(input_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3564f483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relu\n",
      "Expected =[[1 2 3],[4 5 6]]\n",
      "Result = [[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print('Relu')\n",
    "print('Expected =[[1 2 3],[4 5 6]]')\n",
    "print(f'Result = {ReLU_inst.forward(input_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4e595d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax\n",
      "Expected = [[0.09003057 0.24472847 0.66524096],[0.09003057 0.24472847 0.66524096]]\n",
      "Result = [[0.09003057 0.24472847 0.66524096]\n",
      " [0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "print('Softmax')\n",
    "print('Expected = [[0.09003057 0.24472847 0.66524096],[0.09003057 0.24472847 0.66524096]]')\n",
    "print(f'Result = {Softmax_inst.forward(input_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e4aa1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax\n",
      "Expected = [[ 0.08192507 -0.02204578 -0.05987929],[-0.02204578  0.18483645 -0.16279067],[-0.05987929 -0.16279067  0.22266996]]\n",
      "Result = [[[ 0.08192507 -0.02203304 -0.05989202]\n",
      "  [-0.02203304  0.18483645 -0.1628034 ]\n",
      "  [-0.05989202 -0.1628034   0.22269543]]\n",
      "\n",
      " [[ 0.08192507 -0.02203304 -0.05989202]\n",
      "  [-0.02203304  0.18483645 -0.1628034 ]\n",
      "  [-0.05989202 -0.1628034   0.22269543]]]\n"
     ]
    }
   ],
   "source": [
    "#Testing softmax backwards pass\n",
    "print('Softmax')\n",
    "print('Expected = [[ 0.08192507 -0.02204578 -0.05987929],[-0.02204578  0.18483645 -0.16279067],[-0.05987929 -0.16279067  0.22266996]]')\n",
    "print(f'Result = {Softmax_inst.backward()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
