{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523ec949",
   "metadata": {},
   "source": [
    "# Linear, Convolutional,Pooling and Flattening layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ff137ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fdfcb7",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self,in_nodes,out_nodes,lr=0.001):\n",
    "        self.in_ = in_nodes\n",
    "        self.out_ = out_nodes\n",
    "        # self.weight_ = np.random.normal(size=(in_nodes,out_nodes))\n",
    "        fan_in_linear = in_nodes\n",
    "        std_dev_linear = np.sqrt(2.0 / fan_in_linear)\n",
    "        self.weight_ = np.random.normal(loc=0, scale=std_dev_linear, size=(in_nodes,out_nodes))\n",
    "        self.bias = np.zeros((1,out_nodes))+0.01\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self,input_vector):\n",
    "        self.input = input_vector\n",
    "        #output is data@Weight + bias.\n",
    "        self.output = (input_vector @ self.weight_) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def  backward(self, error_signal):\n",
    "        dw = self.input.T @ error_signal\n",
    "        db = np.sum(error_signal,axis=0,keepdims=True)\n",
    "        error_signal = error_signal @ self.weight_.T\n",
    "        #updates\n",
    "        self.weight_ = self.weight_ - self.lr*dw\n",
    "        self.bias = self.bias - self.lr*db\n",
    "        return error_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb70035",
   "metadata": {},
   "source": [
    "### Testing Linear Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003c614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear forward:[[-0.07134928  5.58478674]\n",
      " [-1.33163399 12.37000788]] then activation = [[-0.07122845  0.99997181]\n",
      " [-0.86964812  1.        ]]\n",
      "activation bakc = [[9.94926507e-01 5.63850046e-05]\n",
      " [2.43712144e-01 7.20457027e-11]] then linear back = [[ 0.38643751 -0.34390779 -0.46036575]\n",
      " [ 0.09465342 -0.08424917 -0.11278647]]\n"
     ]
    }
   ],
   "source": [
    "from activations import Sigmoid,ReLU,Tanh\n",
    "\n",
    "input_data = np.array([[1,2,3],[4,5,6]])\n",
    "\n",
    "ReLU_inst = Tanh()\n",
    "Linear_inst = Linear(3,2)\n",
    "Linear_forw = Linear_inst.forward(input_data)\n",
    "activation = ReLU_inst.forward(Linear_forw)\n",
    "print(f'Linear forward:{Linear_forw} then activation = {activation}')\n",
    "back1 = ReLU_inst.backward()\n",
    "back2 = Linear_inst.backward(back1)\n",
    "print(f'activation bakc = {back1} then linear back = {back2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f8793",
   "metadata": {},
   "source": [
    "## Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f03a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv2d is being designed to work with square and rectangular matrices.\n",
    "class Conv2d():\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride,padding=0,lr=0.01):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        # self.kernels = np.random.normal(size=(out_channels,in_channels,kernel_size,kernel_size))\n",
    "        fan_in_conv = in_channels * kernel_size * kernel_size\n",
    "        std_dev_conv = np.sqrt(2.0 / fan_in_conv)\n",
    "        self.kernels = np.random.normal(loc=0, scale=std_dev_conv, size=(out_channels,in_channels,kernel_size,kernel_size))\n",
    "        self.bias = np.zeros((out_channels,1,1))+0.01\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.lr = lr\n",
    "\n",
    "    def convolve(self,input_data,kernels,padding,stride,out_channels,bias=False):\n",
    "        # #conversion to int as np.floor returns a float while indexing requires int.\n",
    "        out_H = int(np.floor(((input_data.shape[2] + 2*padding - kernels.shape[2])/stride))+1)\n",
    "        out_W = int(np.floor(((input_data.shape[3] + 2*padding - kernels.shape[2])/stride))+1)\n",
    "        output_tensor = np.zeros(shape=(input_data.shape[0],out_channels,out_H,out_W))\n",
    "        if padding != 0:\n",
    "            tensor = np.pad(input_data,pad_width=((0,0),(0,0),(padding,padding),(padding,padding)),mode='constant',constant_values=0)\n",
    "        else:\n",
    "            tensor = input_data\n",
    "        #travel path is (0,0)->till right_edge->to(kernel_size,0)->right_edge and repeat.\n",
    "        for batch_id in range(len(tensor)):\n",
    "            for f in range(out_channels):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        #iterators represent the starting point of the kernel on the input matrix.\n",
    "                        matrix = kernels[f,:,:,:] * tensor[batch_id,:,i*stride:i*stride+kernels.shape[2],j*stride:j*stride+kernels.shape[2]]\n",
    "                        output_tensor[batch_id,f,i,j] = np.sum(matrix)\n",
    "            if bias is True:\n",
    "                output_tensor[batch_id,:,:,:] = output_tensor[batch_id,:,:,:] + self.bias\n",
    "        return output_tensor\n",
    "    \n",
    "\n",
    "    def forward(self,input_data):\n",
    "        # #input data is Batch_size,In_channels,H,W\n",
    "        self.input = input_data\n",
    "        self.output = self.convolve(input_data,self.kernels,self.padding,self.stride,self.out_channels,bias=True)\n",
    "        return self.output\n",
    "    \n",
    "    def upsample(self,signal):\n",
    "        H_out = signal.shape[2]\n",
    "        W_out = signal.shape[3]\n",
    "        H_upsampled = (H_out - 1) * self.stride + 1\n",
    "        W_upsampled = (W_out - 1) * self.stride + 1\n",
    "        upsampled_error_signal = np.zeros((signal.shape[0],signal.shape[1],H_upsampled,W_upsampled), dtype=signal.dtype)\n",
    "        upsampled_error_signal[:,:,::self.stride,::self.stride] = signal\n",
    "        return upsampled_error_signal\n",
    "\n",
    "\n",
    "    \n",
    "    def backward(self,error_signal):\n",
    "        #error signal is a [batch_size,out_channels,out_dim,out_dim] what you basically have is pd of E wrt to O\n",
    "        #computing the error signal to back propogate\n",
    "        padding_dim = self.kernel_size - 1 - self.padding\n",
    "        flipped_kernels = (self.kernels[:,:,::-1,::-1]).transpose(1,0,2,3)\n",
    "        if self.stride>1:\n",
    "            dXerror_signal = self.upsample(error_signal)\n",
    "        else:\n",
    "            dXerror_signal = error_signal\n",
    "\n",
    "        new_errorsignal = self.convolve(dXerror_signal,flipped_kernels,padding=padding_dim,stride=1,out_channels=self.in_channels)\n",
    "\n",
    "        diff_H = self.input.shape[2] - new_errorsignal.shape[2]\n",
    "        diff_W = self.input.shape[3] - new_errorsignal.shape[3]\n",
    "\n",
    "        if diff_H > 0 or diff_W > 0:\n",
    "            pad_H_before = diff_H // 2\n",
    "            pad_H_after = diff_H - pad_H_before\n",
    "            pad_W_before = diff_W // 2\n",
    "            pad_W_after = diff_W - pad_W_before\n",
    "            new_errorsignal = np.pad(new_errorsignal,pad_width=((0,0),(0,0),(pad_H_before,pad_H_after),(pad_W_before,pad_W_after)),mode='constant',constant_values=0)\n",
    "        elif diff_H < 0 or diff_W < 0:\n",
    "            crop_H_start = -diff_H // 2\n",
    "            crop_H_end = new_errorsignal.shape[2] - (-diff_H - crop_H_start)\n",
    "            crop_W_start = -diff_W // 2\n",
    "            crop_W_end = new_errorsignal.shape[3] - (-diff_W - crop_W_start)\n",
    "            \n",
    "            new_errorsignal = new_errorsignal[:, :, crop_H_start:crop_H_end, crop_W_start:crop_W_end]\n",
    "\n",
    "        new_errorsignal = new_errorsignal[:, :, :self.input.shape[2], :self.input.shape[3]]\n",
    "        #weight_update.\n",
    "        #dw\n",
    "        out_H = error_signal.shape[2]\n",
    "        out_W = error_signal.shape[3]\n",
    "\n",
    "        if self.padding != 0:\n",
    "            tensor = np.pad(self.input,pad_width=((0,0),(0,0),(self.padding,self.padding),(self.padding,self.padding)),mode='constant',constant_values=0)\n",
    "        else:\n",
    "            tensor = self.input\n",
    "\n",
    "        dw = np.zeros(shape=(self.out_channels,self.in_channels,self.kernel_size,self.kernel_size))\n",
    "        #error signal is B,O,H,W\n",
    "        #tensor is input and is B,I,H,W\n",
    "        #dw is O,I,H,W as kernel is O,I,H,W\n",
    "        for batch_id in range(error_signal.shape[0]):\n",
    "            for out_filter in range(error_signal.shape[1]):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        output_map = error_signal[batch_id,out_filter,i,j]\n",
    "                        matrix = output_map*tensor[batch_id,:,i*self.stride:i*self.stride+self.kernel_size,j*self.stride:j*self.stride+self.kernel_size]\n",
    "                        dw[out_filter,:,:,:] = dw[out_filter,:,:,:] + matrix\n",
    "        \n",
    "        dw = dw/tensor.shape[0]\n",
    "\n",
    "        db = np.sum(error_signal,axis=(0,2,3),keepdims=True)\n",
    "        self.kernels = self.kernels - self.lr*dw\n",
    "        self.bias = self.bias - self.lr*db\n",
    "\n",
    "        return new_errorsignal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679dc01a",
   "metadata": {},
   "source": [
    "### Testing convolution forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f059f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 7, 5)\n",
      "(2, 12, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "# square input data\n",
    "input_data1 = np.random.normal(size=(2,16,9,7))\n",
    "#rectangular input data\n",
    "input_data2 = np.random.normal(size=(2,16,4,7))\n",
    "Conv_inst = Conv2d(16,12,3,1)\n",
    "result1 = Conv_inst.forward(input_data1)\n",
    "result2 = Conv_inst.forward(input_data2)\n",
    "print(result1.shape)\n",
    "print(result2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd425ed",
   "metadata": {},
   "source": [
    "### Testing convolution backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6eb98648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3, 3)\n",
      "(2, 16, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "#Forward pass and instance created.\n",
    "input_data = np.random.normal(size=(2,16,7,7))\n",
    "Conv_inst = Conv2d(16,3,3,3,1)\n",
    "result = Conv_inst.forward(input_data)\n",
    "print(result.shape)\n",
    "\n",
    "#creating a dummy error signal\n",
    "error = np.random.normal(size=(2,3,3,3))\n",
    "back_error = Conv_inst.backward(error)\n",
    "print(back_error.shape)\n",
    "\n",
    "#Shapes are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c2043",
   "metadata": {},
   "source": [
    "## Flattening layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        self.input_dim = None\n",
    "    \n",
    "    def forward(self,tensor):\n",
    "        #tensor shape is B,C,H,W\n",
    "        #output shape is B,C*H*W\n",
    "        self.input_dim = (tensor.shape[1],tensor.shape[2],tensor.shape[3])\n",
    "        matrix = tensor.reshape(tensor.shape[0],-1)\n",
    "        return matrix\n",
    "    \n",
    "    def backwards(self,matrix):\n",
    "        tensor = matrix.reshape(matrix.shape[0],self.input_dim[0],self.input_dim[1],self.input_dim[2])\n",
    "        return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2bd691",
   "metadata": {},
   "source": [
    "### Testing Flattening Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2bd9e8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6, 7, 5)\n",
      "result of flattening = from (2, 6, 7, 5) to (2, 210)\n",
      "result of unflattening = from (2, 210) to (2, 6, 7, 5)\n"
     ]
    }
   ],
   "source": [
    "#Conv2d instance and forward pass.\n",
    "input_data = np.random.normal(size=(2,16,9,7))\n",
    "Conv_inst = Conv2d(16,6,3,1)\n",
    "result = Conv_inst.forward(input_data1)\n",
    "print(result.shape)\n",
    "flatten = Flatten()\n",
    "forward = flatten.forward(result)\n",
    "print(f'result of flattening = from {result.shape} to {forward.shape}')\n",
    "unflatten = flatten.backwards(forward)\n",
    "print(f'result of unflattening = from {forward.shape} to {unflatten.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40178c",
   "metadata": {},
   "source": [
    "### Pooling layer (only Maxpool2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61defc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Pooling():\n",
    "#     def __init__(self,kernel_size,padding,stride):\n",
    "#         self.mask = None\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.padding = padding\n",
    "#         self.stride =stride\n",
    "#         self.kernel = np.ones((kernel_size,kernel_size))\n",
    "\n",
    "#     def forward(self,tensor):\n",
    "\n",
    "#         kernel_tensor = self.kernel[np.newaxis,np.newaxis,:,:]\n",
    "#         kernel_tensor = np.tile(kernel_tensor,(tensor.shape[0],tensor.shape[1],1,1))\n",
    "#         #kernel tensor is B,C,KH,KW\n",
    "\n",
    "#         #the output dims after pooling same mathematically as conv in terms of output dims\n",
    "#         out_H = int(np.floor(((tensor.shape[2] + 2*self.padding - self.kernel_size)/self.stride)+1))\n",
    "#         out_W = int(np.floor(((tensor.shape[3] + 2*self.padding - self.kernel_size)/self.stride)+1))\n",
    "\n",
    "#         output_tensor = np.zeros((tensor.shape[0],tensor.shape[1],out_H,out_W))\n",
    "#         self.mask = np.zeros((tensor.shape[0],tensor.shape[1],tensor.shape[2],tensor.shape[3]))\n",
    "\n",
    "#         if self.padding != 0:\n",
    "#             tensor = np.pad(tensor,pad_width=((0,0),(0,0),(self.padding,self.padding),(self.padding,self.padding)),mode='constant',constant_values=0)\n",
    "        \n",
    "#         for i in range(out_H):\n",
    "#             for j in range(out_W):\n",
    "#                 matrix_tensor = kernel_tensor[:,:,:,:]*tensor[:,:,i*self.stride:i*self.stride+self.kernel_size,j*self.stride:j*self.stride+self.kernel_size]\n",
    "#                 max_values = np.max(matrix_tensor,axis=(2,3),keepdims=True)\n",
    "#                 output_tensor[:,:,i,j] = max_values\n",
    "#                 flattened_view = matrix_tensor.reshape((tensor.shape[0],tensor.shape[1],self.kernel_size**2))\n",
    "#                 rows,cols = np.unravel_index(flattened_view,(self.kernel_size,self.kernel_size))\n",
    "#                 rows = rows + i*self.stride\n",
    "#                 cols = cols + i*self.stride\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
