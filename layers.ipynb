{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523ec949",
   "metadata": {},
   "source": [
    "# Linear and Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ff137ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fdfcb7",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23f6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self,in_nodes,out_nodes,lr=0.001):\n",
    "        self.in_ = in_nodes\n",
    "        self.out_ = out_nodes\n",
    "        self.weight_ = np.random.normal(size=(in_nodes,out_nodes))\n",
    "        self.bias = np.random.normal(size=(1,out_nodes))\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self,input_vector):\n",
    "        self.input = input_vector\n",
    "        #output is data@Weight + bias.\n",
    "        self.output = (input_vector @ self.weight_) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def  backward(self, error_signal):\n",
    "        dw = self.input.T @ error_signal\n",
    "        db = np.sum(error_signal,axis=0,keepdims=True)\n",
    "        error_signal = error_signal @ self.weight_.T\n",
    "        #updates\n",
    "        self.weight_ = self.weight_ - self.lr*dw\n",
    "        self.bias = self.bias - self.lr*db\n",
    "        return error_signal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb70035",
   "metadata": {},
   "source": [
    "### Testing Linear Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003c614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear forward:[[-0.07134928  5.58478674]\n",
      " [-1.33163399 12.37000788]] then activation = [[-0.07122845  0.99997181]\n",
      " [-0.86964812  1.        ]]\n",
      "activation bakc = [[9.94926507e-01 5.63850046e-05]\n",
      " [2.43712144e-01 7.20457027e-11]] then linear back = [[ 0.38643751 -0.34390779 -0.46036575]\n",
      " [ 0.09465342 -0.08424917 -0.11278647]]\n"
     ]
    }
   ],
   "source": [
    "from activations import Sigmoid,ReLU,Tanh\n",
    "\n",
    "input_data = np.array([[1,2,3],[4,5,6]])\n",
    "\n",
    "ReLU_inst = Tanh()\n",
    "Linear_inst = Linear(3,2)\n",
    "Linear_forw = Linear_inst.forward(input_data)\n",
    "activation = ReLU_inst.forward(Linear_forw)\n",
    "print(f'Linear forward:{Linear_forw} then activation = {activation}')\n",
    "back1 = ReLU_inst.backward()\n",
    "back2 = Linear_inst.backward(back1)\n",
    "print(f'activation bakc = {back1} then linear back = {back2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f8793",
   "metadata": {},
   "source": [
    "## Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f03a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv2d is being designed to work with square and rectangular matrices.\n",
    "class Conv2d():\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride,padding=0,lr=0.01):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernels = np.random.normal(size=(out_channels,in_channels,kernel_size,kernel_size))\n",
    "        self.bias = np.random.normal(size=(out_channels,1,1))\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.lr = lr\n",
    "\n",
    "    def convolve(self,input_data,kernels,padding,stride,out_channels,bias=False):\n",
    "        # #conversion to int as np.floor returns a float while indexing requires int.\n",
    "        out_H = int(np.floor(((input_data.shape[2] + 2*padding - kernels.shape[2])/stride)+1))\n",
    "        out_W = int(np.floor(((input_data.shape[3] + 2*padding - kernels.shape[2])/stride)+1))\n",
    "        output_tensor = np.zeros(shape=(input_data.shape[0],out_channels,out_H,out_W))\n",
    "        if padding != 0:\n",
    "            tensor = np.pad(input_data,pad_width=((0,0),(0,0),(padding,padding),(padding,padding)),mode='constant',constant_values=0)\n",
    "        else:\n",
    "            tensor = input_data\n",
    "        #travel path is (0,0)->till right_edge->to(kernel_size,0)->right_edge and repeat.\n",
    "        for batch_id in range(len(tensor)):\n",
    "            for f in range(out_channels):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        #iterators represent the starting point of the kernel on the input matrix.\n",
    "                        matrix = kernels[f,:,:,:] * tensor[batch_id,:,i*stride:i*stride+kernels.shape[2],j*stride:j*stride+kernels.shape[2]]\n",
    "                        output_tensor[batch_id,f,i,j] = np.sum(matrix)\n",
    "            if bias is True:\n",
    "                output_tensor[batch_id,:,:,:] = output_tensor[batch_id,:,:,:] + self.bias\n",
    "        return output_tensor\n",
    "    \n",
    "\n",
    "    def forward(self,input_data):\n",
    "        # #input data is Batch_size,In_channels,H,W\n",
    "        self.input = input_data\n",
    "        self.output = self.convolve(input_data,self.kernels,self.padding,self.stride,self.out_channels,bias=True)\n",
    "        return self.output\n",
    "    \n",
    "    def upsample(self,signal):\n",
    "        H_out = signal.shape[2]\n",
    "        W_out = signal.shape[3]\n",
    "        H_upsampled = (H_out - 1) * self.stride + 1\n",
    "        W_upsampled = (W_out - 1) * self.stride + 1\n",
    "        upsampled_error_signal = np.zeros((signal.shape[0],signal.shape[1],H_upsampled,W_upsampled), dtype=signal.dtype)\n",
    "        upsampled_error_signal[:,:,::self.stride,::self.stride] = signal\n",
    "        return upsampled_error_signal\n",
    "\n",
    "\n",
    "    \n",
    "    def backward(self,error_signal):\n",
    "        #error signal is a [batch_size,out_channels,out_dim,out_dim] what you basically have is pd of E wrt to O\n",
    "        #computing the error signal to back propogate\n",
    "        padding_dim = self.kernel_size - 1\n",
    "        flipped_kernels = (self.kernels[:,:,::-1,::-1]).transpose(1,0,2,3)\n",
    "        if self.stride>1:\n",
    "            dXerror_signal = self.upsample(error_signal)\n",
    "        else:\n",
    "            dXerror_signal = error_signal\n",
    "\n",
    "        new_errorsignal = self.convolve(dXerror_signal,flipped_kernels,padding=padding_dim,stride=1,out_channels=self.in_channels)\n",
    "        #weight_update.\n",
    "        #dw\n",
    "\n",
    "        out_H = error_signal.shape[2]\n",
    "        out_W = error_signal.shape[3]\n",
    "\n",
    "        if self.padding != 0:\n",
    "            tensor = np.pad(self.input,pad_width=((0,0),(0,0),(self.padding,self.padding),(self.padding,self.padding)),mode='constant',constant_values=0)\n",
    "        else:\n",
    "            tensor = self.input\n",
    "\n",
    "        dw = np.zeros(shape=(self.out_channels,self.in_channels,self.kernel_size,self.kernel_size))\n",
    "        #error signal is B,O,H,W\n",
    "        #tensor is input and is B,I,H,W\n",
    "        #dw is O,I,H,W as kernel is O,I,H,W\n",
    "        for batch_id in range(error_signal.shape[0]):\n",
    "            for out_filter in range(error_signal.shape[1]):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        output_map = error_signal[batch_id,out_filter,i,j]\n",
    "                        matrix = output_map*tensor[batch_id,:,i*self.stride:i*self.stride+self.kernel_size,j*self.stride:j*self.stride+self.kernel_size]\n",
    "                        dw[out_filter,:,:,:] = dw[out_filter,:,:,:] + matrix\n",
    "        \n",
    "        dw = dw/tensor.shape[0]\n",
    "\n",
    "        db = np.sum(error_signal,axis=(0,2,3),keepdims=True)\n",
    "        self.kernels = self.kernels - self.lr*dw\n",
    "        self.bias = self.bias - self.lr*db\n",
    "\n",
    "        return new_errorsignal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679dc01a",
   "metadata": {},
   "source": [
    "### Testing convolution forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f059f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 7, 5)\n",
      "(2, 12, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "# square input data\n",
    "input_data1 = np.random.normal(size=(2,16,9,7))\n",
    "#rectangular input data\n",
    "input_data2 = np.random.normal(size=(2,16,4,7))\n",
    "Conv_inst = Conv2d(16,12,3,1)\n",
    "result1 = Conv_inst.forward(input_data1)\n",
    "result2 = Conv_inst.forward(input_data2)\n",
    "print(result1.shape)\n",
    "print(result2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd425ed",
   "metadata": {},
   "source": [
    "### Testing convolution backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb98648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 7, 5)\n",
      "(2, 16, 9, 7)\n"
     ]
    }
   ],
   "source": [
    "#Forward pass and instance created.\n",
    "input_data = np.random.normal(size=(2,16,9,7))\n",
    "Conv_inst = Conv2d(16,12,3,1)\n",
    "result = Conv_inst.forward(input_data1)\n",
    "print(result.shape)\n",
    "\n",
    "#creating a dummy error signal\n",
    "error = np.random.normal(size=(2,12,7,5))\n",
    "back_error = Conv_inst.backward(error)\n",
    "print(back_error.shape)\n",
    "\n",
    "#Shapes are correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
